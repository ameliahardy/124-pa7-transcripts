#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): NO
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: NO
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: YES
FEATURE - Dialogue for disambiguation: YES
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: NO
FEATURE - Responding to arbitrary input: YES
FEATURE - Identifying and responding to emotions: YES
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################
TODO: Melissa, Justin, Meera, and Jiwon all worked on implementation of creative features of process(text). 
Melissa completed all of standard mode. Meera worked on creative features for find_movies_by_title.
Meera and Justin worked on find_movies_closest_to_title and disambiguate part 2.
Jiwon worked on creative sentiment extraction and the ethics questions. 


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################
TODO: Please answer the following question:

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

Some possible ramifications of anthropomorphizing chatbot systems is that the more elaborate they get, the more humans start trusting them and expecting human-like responses from them. Because of this, humans may perceive chatbot responses as accurate and sometimes even better than human responses, although chatbot systems run on limited resources and limited responses. Especially in nuanced situations such as when chatbots have to make decisions or respond to a human's emotions, their choices and responses may differ from those of humans. Anthropomorphization of chatbots also raises several ethical concerns. For example, while all humans have their own identity, biases, and backgrounds, chatbots do not have an identity or a certain background. Instead, they are programmed and run on data that is given to them. This could prevent them from making decisions or responsing appropriately with the right context. 

Some ways that engineers could ensure that users can easily distinguish chatbot responses from those of a human are to clearly outline the limitations of the chatbot to the users. This transparency could prevent users from setting unrealistic expectations for the chatbots and relying too much on them. Another way to help users distinguish is to clearly label the chatbot so the users know that they are talking to a chatbot instead of a human being. This assumption could help users determine which questions they want to ask the chatbot and reaffirm that they are talking to a non-human. Lastly, engineers could provide an option for the users to talk to a human, if possible. This helps the users remember that talking to a chatbot is different from talking to a human.  

#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!
