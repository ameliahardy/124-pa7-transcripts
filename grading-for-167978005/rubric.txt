#########################################################################################
# Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################


FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: YES
FEATURE - Dialogue for disambiguation: YES
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: YES
FEATURE - Responding to arbitrary input: NO
FEATURE - Identifying and responding to emotions: NO
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################
Please write a short description of what each team member contributed!

Austin Salcedo:
Identifying movies without quotation marks and correct capitalization (part 1)
Identifying movies without quotation marks and correct capitalization (part 2)
Alternate/foreign titles
Fine-grained sentiment extraction

Helped with writing Creative process


Anthony Argyropoulos:
Disambiguation (part 1): YES
Spell-correcting fallback for find_movies_by_title: YES
Disambiguation (part 2)
Dialogue for disambiguation

Wrote the description part of the bot

Rosman Carino:
Extracting sentiment with multiple-movie input
Dialogue for spell-checking
Communicating sentiments and movies extracted to the user given multiple-movie input
Chatbot theme/persona

Came up with the name for our bot 

Zouberou Sayibou:
Wrote all the starter code functions
Helped with writing process function


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################
Please answer the following question:

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

This is a complex and multifaceted ethical discussion that is reminiscent of the question of whether the use
of humanoid robots in elderly care facilities as social companions is ethical. There are important philosophical
questions at the root of this issue. Is it wrong to deceive someone if that deceit brings them satisfaction and 
happiness? I would argue yes because we have a slight moral responsibility to live unimpaired. But I argue from 
the perspective of someone who hasn't lived in the deep loneliness that an elderly person can often experience in 
elderly care. So if we make it known in advance that they are not interacting with another conscious being but that 
thing (like Eliza) is designed to present like a human, there are subconscious processes at play that make it easy 
to forget. That’s why I believe there is no good way to distinguish the chatbot responses from those of a human if 
the goals is to make the responses appear natural and human like. There are however, other interfaces that could serve 
the same purpose on recommending movies, without seeming humanlike.

#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!
