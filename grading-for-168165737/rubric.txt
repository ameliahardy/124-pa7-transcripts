#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES 
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: NO
FEATURE - Dialogue for disambiguation: NO
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: YES
FEATURE - Responding to arbitrary input: YES
FEATURE - Identifying and responding to emotions: NO
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################
Nikesh and Emma worked on preprocess. Jasper primarily worked on process, with some help
from Nourya and a little bit of help from Emma. Nikesh worked on extract titles and find movies by title.
Emma and Nourya worked on extract sentiment. Emma worked on extract sentiment for movies, with some help 
from Nourya. Nikesh worked on find movies closest to title, disambiguate, binarize, similarity, and recommend. 
Emma wrote up the ethics question, and the other teammates approved of it.


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################


Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

There are many consequences to anthropomprhizing chatbot systems. First, if the chatbot is not sufficiently 
tested, it may be able to manipulate or exploit humans. There was a popular example of this occurring recently
with Bing's chatbot "Sydney" after it released, you can view it in the New York Times article "A Conversation With Bing’s 
Chatbot Left Me Deeply Unsettled". In addition to being manipulated or exploited, chatbots can get facts wrong without 
properly caveating that they are not confident in their answer. This can lead to humans trusting the chatbots and changing their
behavior based on incorrect information. It's important to not trust a chatbot as one would a human.
Additionally, this can lead to individuals sharing private data, such as passwords, addresses, or other personal information.
If the chatbot is trained in the future on individuals' answers, this may lead to data leaks if not dealt with carefully.

In order for users to distinguish the chatbot's responses from that of a human, engineers can do a few things. First, they can
properly caveat what the chatbot's limitations and where it may be wrong. Engineers can be transparent about how the chatbot was trained,
and say that there are likely still flaws with the system. Engineers can say how the chatbot should and should not be used,
and convey that the chatbot does not have emotions or the power to reason on its own, but is instead the product of "training" on lots of 
language data sets from X, Y, and Z sources. 

#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!