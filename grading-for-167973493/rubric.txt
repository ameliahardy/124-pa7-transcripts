#########################################################################################                                                                                    #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): NO
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): NO
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: YES
FEATURE - Dialogue for disambiguation: YES
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: YES
FEATURE - Responding to arbitrary input: NO
FEATURE - Identifying and responding to emotions: NO
FEATURE - Chatbot theme/persona: NO
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################
Eban: Extract titles, creative find_movies_by_title, recommend, find_movies_closest_to_title
Ossose: Binarize, disambiguate, creative extract sentiment, ethics
Chase: find_movies_by_title, process, creative process disambiguate
Mack: extract sentiment for movies, standard extract sentiment


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################
Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

Anthropomorphizing technology or chatbots like ELIZA can have potentially harmful 
ramifications. As it was stated in the prompt, sometimes these neural-network trained
chatbots are not correct in their assertions, and can even spew violent or hateful speech. 
If someone using the chatbot were to give that chatbot the same amount of ethos that
might be attributed to a human expert in particular field or even a friend, it can lead to 
the belief in erroneous answers (users challenge chatbots' answers less) and the
normalization of hatful, violent, and dangerous language. 

With artifical intelligence, we engineers have a unique obligation to inform 
users of our technologies the dangers of taking statements from these LLMs as
fact. Furthermore, we also have an obligation to thoroughly and ethically test 
these LLMs to remove as much misinformation and hate speech as possible. 
With the testing of GPT-3 in order to screen out graphic content, workers form 
the global south were underpaid, overworked, and not given adequate conseling when 
being asked to label dark and graphic material. This method of testing/filtering 
was not ethical and had detrimental mental-health consequences for labelers/workers. 
It is so important to source ethical human labor when it comes to improving these 
LLMs or any technology. These technologies impact so many people; therefore, as 
engineers, it is our responsibility, through testing, disclaimers, and filtering, 
to mitigate the causes and effects of anthropomorphizing AI. 


#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################

