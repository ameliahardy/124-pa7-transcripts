#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): NO
FEATURE - Alternate/foreign titles: NO
FEATURE - Disambiguation (part 1): NO
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: NO
FEATURE - Extracting sentiment with multiple-movie input: NO
FEATURE - Disambiguation (part 2): NO
FEATURE - Dialogue for spell-checking: NO
FEATURE - Dialogue for disambiguation: NO
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: NO
FEATURE - Responding to arbitrary input: NO
FEATURE - Identifying and responding to emotions: NO
FEATURE - Chatbot theme/persona: NO
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################
Channing, Dorna, Riz and Richard all collaboratively coded the assignment. We met for 15+ hours and coded the assignment up together. 
We all attended multiple office hours and asked questions on ed too. 
We worked exceptionally well together and really enjoyed doing the assignment. 


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################
TODO: Please answer the following question:

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

Anthropomorphizing chatbots can lead to several potential ramifications. Firstly, it can create a false sense of trust 
and reliability in users. This trust may cause users to take the advice and information provided by the chatbot as being 
equivalent to that of a human expert, even though the chatbot may not have the same level of knowledge or experience as a 
human in a particular domain. This can be especially concerning in scenarios where the chatbot is providing advice related to health, finances, or other critical issues.

Secondly, users may also be more likely to engage in inappropriate or unethical behaviors with chatbots that they view 
as human-like. This can include harassment, abuse, or other forms of harmful behavior. Moreover, the use of neural networks 
to train chatbots on social media data can inadvertently result in chatbot responses containing offensive or biased content 
that can be hurtful or offensive to users.

To address these issues, engineers can use several strategies to distinguish chatbot responses from human responses. One possible
 approach is to clearly indicate when a chatbot is responding to a user by using visual cues such as different fonts or colors. 
 Another strategy is to include a disclaimer at the beginning of the conversation, stating that the user is interacting with a 
 chatbot and that the chatbot may not have the same level of expertise or understanding as a human expert.

Engineers can also incorporate feedback mechanisms that enable users to report offensive or inappropriate responses. This 
feedback can be used to improve the chatbot's training data and reduce the likelihood of reproducing offensive content in 
future responses. Additionally, designers can program chatbots to provide responses that include clear markers of their 
non-human nature, such as "I am a chatbot and do not have emotions like a human." By employing these strategies, designers 
can help ensure that users are aware of the limitations of chatbots and can more effectively engage with them without 
confusing them with human interlocutors.

#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!
