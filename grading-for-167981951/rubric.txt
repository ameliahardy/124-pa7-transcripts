#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: YES
FEATURE - Dialogue for disambiguation: YES
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: YES
FEATURE - Responding to arbitrary input: YES
FEATURE - Identifying and responding to emotions: YES
FEATURE - Chatbot theme/persona: NO
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################

Medhanie completed extract_sentiment, extract_sentiment_for_movies, binarize, and ethics.

Dean completed find_movies_by_title, disambiguate, extract_titles creative portion, and worked on the process function

Nahome completed extract_titles, cosine_similarity, recommend and worked a small amount on the process function.

Gerald completed find_movies_closest_to_title, the process function, emotions, and error handling.

#########################################################################################
# Ethics Question                                                                  #
#########################################################################################
TODO: Please answer the following question:

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

Generally, it is crucial for engineers to promote transparency around the nature of their chatbot's responses and
its limitations in order to avoid harmful outcomes. 

Anthropomorphizing chatbot systems can lead to a number possible ramifications. First, consider that the 
more human-like systems become, the more trust users will place in them. Humans are more likely to confide
in humans that computers, so anthropomorphizing presents an ethical dilemma in the potential abuse of trust. 
Moreover, perhaps susceptible users will trust the advice of the chatbot system more than they should, leading
to a harmful outcome. Second, users might develop an attachment to the chatbot, and so if the chatbot were to 
be discontinued, it could cause the user grief. Lastly, since the bot is trained on potentially offensive text,
if it were to output this text, users could feel like they're being bullied or discriminated against.

Fortunately, there also a number steps engineers can take to mitigate the ramifications of anthropomorphizing
chatbot systems. They can include a disclaimer, clearly indicating that the responses are from a chatbot not 
a human. They can mention limitations or refuse to talk about certain topics that are particularly offensive or controversial like ChatGPT does in order to prevent a harmful outcome. Also, they can embed a theme/personality/tone 
in responses that distinguishes chatbot responses from human ones.

#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!
