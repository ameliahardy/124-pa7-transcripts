#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: NO
FEATURE - Dialogue for disambiguation: NO
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: YES
FEATURE - Responding to arbitrary input: NO
FEATURE - Identifying and responding to emotions: NO
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################

Kayson - wrote extract_titles, find_movies_by_title, binarize, extract_sentiment_for_movies, disambiguation
Emily - wrote extract_sentiment, starter mode process, creative mode process
Iddah - wrote recommend, creative mode extract_titles, creative mode find_movies_by_title
David - wrote part of extract_sentiment and extract_sentiment's creative extension

#########################################################################################
# Ethics Question                                                                  #
#########################################################################################

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?


One possible ramification of anthropomorphizing chatbot systems is misleading people about what chatbot systems
actually are. Chatbot systems are computer programs executing pre-programmed instructions; they're not alive,
they're not conscious, and they don't have feelings. For example, chatbot systems like Chat GPT that are trained
using neural network models are essentially gigantic calculators carrying out billions of computations of matrix
multiplication and partial derivatives in order to generate responses through forward-propagation. However, 
because Chat GPT is programmed to respond like a human, it's hard to comprehend what's actually going on behind
the scenes. 

Another possible ramification with anthropomorphizing chatbots is that they could be used to manipulate people.
Just like in the Eliza case where people started to trust Eliza, if someone made a chatbot now that was 
programmed to tell people fake news or other inaccurate information, it could be very convincing. This potential
to spread incorrect or otherwise harmful information is definitely a potential ramification of anthropomorphizing
chatbots.

Another potential ramification would be causing the user to divulge personal information that they wouldn't ordinarily
share with a computer. If a chatbot acted human-like, this could cause someone to trust the chatbot, telling them
person, potentially compromising information that shouldn't be shared publicly. 

One way engineers could ensure that users can easily distinguish the chatbot responses from those of a human are 
adding a disclaimer to the chatbot before the user starts chatting with them that makes it clear that the chatbot
isn't sentient and is just a computer program. Another way is to add some sort of signature to responses, allowing
users to quickly tell if a response is coming from a chatbot or not.

#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!
