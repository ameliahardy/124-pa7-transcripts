######################################################################################### 
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: NO
FEATURE - Spell-correcting fallback for find_movies_by_title: NO
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: YES
FEATURE - Dialogue for disambiguation: NO
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: NO
FEATURE - Responding to arbitrary input: NO
FEATURE - Identifying and responding to emotions: NO
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################
All team members contributed equally. Everyone worked on their own set of rubric elements individually,
and then we combined them all at the end.


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################
Humans are quick to anthropomorphize chatbots, like ELIZA.
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice,
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks,
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

There could be positive and negative ramifications of anthropomorphizing chatbot systems. Human-like chatbot systems could help provide better customer care. For example, an anthropomorphic clothing chatbot system can give great feedback to customers shopping for certain items. Perhaps the more humanistic responses can encourage an individual to purchase clothes to make them feel more confident or try a new dressing style. Anthropomorphic chatbot systems could also provide better mental health support. If someone texted a suicide hotline, a more humanistic chatbot could reply back in a helpful manner. Further, a chatbot would allow the hotline to reach more people.

There are also negative ramification of such human-like chatbot systems. It might be easier to catfish individuals - people might be texting someone on a dating app and think that there is a person on the other end when, in reality, a chatbot has been responding to them. This would bring up very interesting ethical ramifications of if a human fell in love with a chatbot under the assumption that it was a human. There could also be a misunderstanding of the privacy offered by the chatbot system (e.g. if
the user feels like there is a person on the other side of the interaction and not a chatbot, they may not
be aware that their responses are being collected/used to train the bot). Misinformation can also be spread if chatbots are
able to write text that sounds like it could come from a credible source but actually contains false information.

Engineers could ensure that users can distinguish chatbot response from human responses by writing a disclaimer. They can note that all responses from point A to point B are generated by a chatbot system. This could be especially helpful if the service a human is receiving is partially human-generated and chatbot generated. Engineers can also incorporate different UI features to indicate that a response has been generated by a chatbot. The chatbot can clarify each of its responses with as an answer coming from a chatbot (similar to how ChatGPT handles responses) using a statement like "As a chatbot, I do not have preferences...".  

#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #

All team members contributed equally. 

#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!