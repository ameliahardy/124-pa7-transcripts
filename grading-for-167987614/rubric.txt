#########################################################################################
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): MAYBE
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): MAYBE
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: MAYBE
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: MAYBE
FEATURE - Dialogue for disambiguation: MAYBE
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: NO
FEATURE - Responding to arbitrary input: NO
FEATURE - Identifying and responding to emotions: NO
FEATURE - Chatbot theme/persona: MAYBE
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################

Ashley: coded a lot of title extraction, sentiment extraction, & creative functions
Hamed: coded a lot of the intial setup functions, title extraction, & processing function
Parker: coded a lot of the sentiment extraction, processing function, & creative functions

Overall, we all worked together in-person a lot and were helping each other with all of the functions. Really a team effort!

#########################################################################################
# Ethics Question                                                                  #
#########################################################################################

Humans are quick to anthropomorphize chatbots, like ELIZA.
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice,
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks,
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

A major potential ramification of anthropomorphizing chatbots is influencing users in negative ways with the added trust that the
response came from a chatbot. Because responses from chatbots, AI, and computers, in general, are generally considerered to be more reputable
than at least, a random person, harmful responses by a chatbot might have more influence and real-world harm than even a harmful
message from a person online. If chatbots are influenced by these harmful messages in the dataset, they might perpetuate the
harmful messages with even more impact. There are other possible ramifications of anthropomorphizing chatbots and influencing
users to perceive them as human. For example, if a user seeks medical advice, or perhaps therapy from a chatbot, thinking that it
has the expertise to address these issues and responds to them in believable, human-like ways, it might discourage users from
seeking legitimate medical or mental health assistance. There are plenty of hidden ramifications if people use chatbots without
the perception that they are indeed automated and not human responses, and some of them can be very harmful.

One potential way to distinguish chatbot responses from those of a human is by clearly indicating, for example, through the name of the chatbot,
that the chatbot is indeed not human. For example, many companies name their chatbots after real people and given them personas in
a similar style to Siri or Alexa, and when these personas are obscured behind the internet, it could be difficult to tell if they
are a person or not. Another way would by making the chatbot output distinctly nonhuman, like for example, how old versions of
Siri sounded pretty robotic and had odd pacing. Leaning into these distinctly nonhuman characteristics can help distinguish chatbot
responses from those of a human.

#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!