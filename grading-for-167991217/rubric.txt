#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): NO
FEATURE - Alternate/foreign titles: NO
FEATURE - Disambiguation (part 1): NO
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: YES
FEATURE - Dialogue for disambiguation: YES
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: YES
FEATURE - Responding to arbitrary input: YES
FEATURE - Identifying and responding to emotions: YES
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################
Julien: starter and creative portions of find_movies_by_title(), recommend(), and binarize()
Alexis: starter and creative portions of extract_titles() and extract_sentiment()
Mei-Lan: starter portions of process(), extract_sentiment_for_movies(), chatbot persona, ethics question, integration & QA
Jun: creative portions of process(); disambiguate(); and process(), integration & QA


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################
TODO: Please answer the following question:

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

#########################################################################################
# Answer                                                                #
#########################################################################################

### Ramifications of Anthropomorphizing Chatbot Systems ###

Chatbots are purposefully anthropomorphized for ease of interaction; humans can port existing understandings 
of conversation and human conversation to their interactions with the program. But a ramification of that 
choice is that the schema comes with baggage; we also assume, for instance, that people we interact with are 
intentional and reasoning, with a symbolic understanding of the world that lines up with ours. 

Take the example of ChatGPT’s “hallucinations,” wherein the program will confidently make up facts fabricate 
news articles, papers, etc. This behavior is not surprising when you consider that ChatGPT is essentially 
advanced autocorrect; but it runs counter to the schemas and standards of interaction established by the 
combination of its explicit anthropomorphization as a chatbot and its ethos as a machine.

To address the former, anthropomorphizing ChatGPT leads us to think, at least subconsciously, that it has 
some kind of understanding of what it’s saying, that its statements are reflective of an underlying set of values. 
The anthropomorphization divorces us from the underlying mechanism of how ChatGPT works and forces us to make a 
much fuzzier, more human evaluation of ChatGPT. The question is no longer “would word association based on a corpus 
of internet data lead to a useful answer?” It becomes “do I trust ChatGPT?”

That’s where the latter point comes in. In a way, the anthropomorphization of ChatGPT is almost appropriate. Human 
beings are also prone to misremembering fabrications as fact. The problem is that ChatGPT isn’t a human, it is 
explicitly an AI assistant. The machines most of us interact with on a daily basis are highly-specialized tools, 
and we have come to expect precise, mechanical accuracy from them. You would never think, for instance, that your 
calculator might “make a mistake,” so why would ChatGPT? The issue isn’t even necessarily that ChatGPT gets things 
wrong (though obviously it would be better if it didn’t), the issue is that, because of how we’ve been primped to view 
ChatGPT, we don’t think to check.


### Ways to Distinguish ChatBot Systems from Those of a Human ###

It’s not clear to me that lessening the anthropomorphization of chatbots is the best way to move forward. Take the 
example of ELIZA; is ELIZA’s therapeutic advice really so much worse than that of a human therapist? A human therapist 
from the 1960s would likely have advice and views that we’d find morally repugnant in 2023. 

I also don’t buy that it’s inherently a problem that users of ELIZA became emotionally attached to it. Humans develop 
parasocial relationships constantly: we grow emotionally attached to celebrities we’ve never met and fictional characters 
who don’t exist. Even a relationship with a human therapist has an element of this; it would be surprising if someone DIDN’T 
become emotionally attached to the person helping them work through trauma, but the therapist/patient relationship is lopsided 
by definition. 

Having an emotional attachment to an entity that cannot return your affections is not inherently pathological, as long as 
that attachment is paired with an understanding of what constitutes acceptable behavior within the bounds of the reality of 
the relationship. Having a celebrity crush is fine; stalking that celebrity is not. 

The chatbot/human relationship is a relatively new one, and we don’t have strong social scripts in place for how we should 
treat chatbots and what expectations we should have for the things they tell us. In my opinion, the way forward is not to try 
to lesson the anthropomorphization of machines (if we can anthropomorphize the sun, we will find a way to anthropomorphize AI), 
but to establish boundaries for the kind of “person” the chatbot is and what we can expect for it. We don’t expect to completely 
trust or agree with every single thing a friend says, why should it be different with ChatGPT? In a way, it’s only human.


#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!
