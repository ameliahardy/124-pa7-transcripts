#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: YES
FEATURE - Dialogue for disambiguation: YES
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: YES
FEATURE - Responding to arbitrary input: YES
FEATURE - Identifying and responding to emotions: YES
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################

All team members, David Yang, June Lee and I (Alex Paek) contributed fully to the chatbot
assignment. For every single method and feature including the creative ones, we talked
it through as a group and decided on how to implement them. Once a consensus was made,
we split up the functions evenly to maximize efficiency. Once we were done implementing
each function, we git pushed/pulled to have others double check the function and its
functionality. The person who actually wrote the code was able to move onto implementing
the next function only when the other two members fully approved the code.


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################
TODO: Please answer the following question:

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

One possible ramification is people wholeheartedly believing what the chatbot says, leading to the establishment
of skewed perspectives and racial/gender/hierarchical prejudices amongst the users. One example can be children
using the chatbot. As they have not yet developed their own ethic and value principles, and also have limited
knowledge on how chatbots work, they will be extremely prone to the answers produced by chatbots,
possibly leaning to the formation of unethical standards as to how they view certain groups depending on how the chatbot
replies to their questions about the world. Adults should also be wary of using chatbots in a professional setting
as they may produce unintended biases towards a certain demographic. Users may also have higher expectations for
the chatbot as they sound like humans, which would have an unintended negative consequence as the failure for
the chatbot to provide a reasonable answer will lead to greater disappointment and frustration.
There is a positive consequence of anthropomorphizing chatbot systems, which is that users—when used safely—will 
be much more engaged in the chatbot experience, allowing them to be more personal and have a more intimate, valuable
overall experience. This is especially the case when designing a chatbot to act as a mental counselor, where user engagment
becomes a critical factor for success. Even if the chatbot's overall content is the same, users tend to disclose more 
personal infomration if the chatbots are more anthropomorphised.  

One way to ensure the users to distinguish chatbot responses from those of a human is to have a disclaimer at the beginning
of the experience, as regular disclaimers throughout, notifying the user that they are interacting with a chatbot system
and not a real human. Another strong way to ensure the difference is to physically limit the chatbot's capability to 
sound or resemble a human, inhibiting the system from asking very personal questions or displaying true emotions. 
On that note, a slightly more subtle way would be to maintain a very consistent rhetoric for the chatbot system
that differs from a human, that may at times sound robotic.



#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
We decided to carry on a "Sherlock Holmes" persona for our chatbot.


