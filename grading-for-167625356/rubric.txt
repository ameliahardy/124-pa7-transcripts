#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: NO
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: YES
FEATURE - Dialogue for disambiguation: YES
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: NO
FEATURE - Responding to arbitrary input: NO
FEATURE - Identifying and responding to emotions: NO
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################

Anna: extract_titles (starter + creative), find_movies_by_title (starter + creative), recommend, binarize, ethics question
Jackie: process, recommend, dialogue for disambiguation/spell-checking, ethics question
Katy: extract_sentiment (starter + creative), extract_sentiment_for_movies, adding a persona, general debugging, ethics question

#########################################################################################
# Ethics Question                                                                  #
#########################################################################################

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

One possible ramification is that anthropomorphizing chatbots removes the responsibility from the engineer
to ensure that their chatbot is non-biased and appropriate - it makes it seem like any hateful content is
from the 'living' bot instead of the living human that created it. Another is that the anthropomorphization
can cause users to trust that the chatbot is correct even though the algorithm powering it may not handle
edgecases correctly. I know this behavior has been observed in medical imaging machine learning algorithms,
where physicians change their initially correct judgments because the algorithm says something different.
Finally, anthropomorphizing these chatbots can cause users to confide more in the chatbot than they 
necessarily should, which can cause problems when the privacy policy or data use statement is broad. Users
might confide something private but then it might be shared with the whole world.

Some ways that engineers could help users distinguish chatbot responses include coloring the chatbot 
responses differently in the UI compared to any human-generated ones, putting "Chatbot" next to the 
bot's name in any conversations (for example, "Stanley (Chatbot)"), and adding a distinctive persona to 
the chatbot (for example, tending to use a specific level of vocabulary). 


#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!
