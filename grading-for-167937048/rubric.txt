#########################################################################################
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): NO
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: NO
FEATURE - Disambiguation (part 1): NO
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): NO
FEATURE - Dialogue for spell-checking: YES
FEATURE - Dialogue for disambiguation: NO
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: YES
FEATURE - Responding to arbitrary input: YES
FEATURE - Identifying and responding to emotions: YES
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################

We were able to come together in person to discuss and brainstorm the code together. We were tackling the functions 
together and bouncing ideas off each other until we came to a solid function. Fletcher took the lead on recommend, process, 
and user interface; Eliska took the lead with the extract_sentiments and similarity functions; Missy and Jocelyn took the 
lead with the creative functions/components and ethical discussion. Together, we debugged the functions and thoroughly tested 
the user interface.

#########################################################################################
# Ethics Question                                                                  #
#########################################################################################

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human?

Anthropomorphizing chatbots can influence the user in several ways, including but not limited to the user dissociating from
reality and becoming dependent on the chatbot. This can come in the form of emotional attachment, from relying on the chatbot
to provide the user with factual information to using the bot as a form of entertainment. Systems like Alexa, present in the
typical household, have presented concerning recommendations when prompted with entertainment requests. One case involved a
10-year-old girl asking Alexa for a challenge. An assumed response would be to prompt the user to answer a complex math problem,
but instead, Alexa asked the child to touch a penny to an exposed electrical socket. With an increasingly human-like response by
systems like chatbots and Alexa, users will be more susceptible to their requests and opinions and likely be influenced to partake
in questionable and possibly dangerous actions just because the trusted system encouraged it.
Engineers could consider providing a citation when prompted with certain tasks. In the example of the child asking Alexa for a challenge, 
engineers could implement a feature like “According to [Article Title], here are the top 10 challenges trending…” which would supply the 
user with more information about the response. In addition, engineers could opt to reduce the perceived humanity of the chatbot. While it 
is captivating that a chatbot could resemble a human, it would reduce the ramifications by reducing the potential for a human user to become 
dependent on the chatbot. The chatbot could give tailored responses, but refrain from speaking too personally and as “someone” with a 
relationship to the user.

#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
(optional) If you have anything else you want to add, delete this and type it here!
