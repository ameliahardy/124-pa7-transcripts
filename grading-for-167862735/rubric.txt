#########################################################################################
# TODO: Fill this template out in addition to the code implementation in chatbot.py!    #
#                                                                                       #
# Each "Creative" feature in the rubric has a section below.                            #
# For every feature you chose to implement, replace the "NO" at the end of the relevant #
# lines with "YES".                                                                     #
#                                                                                       #
# You will only receive credit for the features you mark as YES below!                  #
#########################################################################################

FEATURE - Identifying movies without quotation marks and correct capitalization (part 1): YES
FEATURE - Identifying movies without quotation marks and correct capitalization (part 2): YES
FEATURE - Alternate/foreign titles: YES
FEATURE - Disambiguation (part 1): YES
FEATURE - Fine-grained sentiment extraction: YES
FEATURE - Spell-correcting fallback for find_movies_by_title: YES
FEATURE - Extracting sentiment with multiple-movie input: YES
FEATURE - Disambiguation (part 2): YES
FEATURE - Dialogue for spell-checking: NO
FEATURE - Dialogue for disambiguation: NO
FEATURE - Communicating sentiments and movies extracted to the user given multiple-movie input: NO
FEATURE - Responding to arbitrary input: NO
FEATURE - Identifying and responding to emotions: NO
FEATURE - Chatbot theme/persona: YES
Did not implement any of the above features: NO

#########################################################################################
# Team Contributions                                                                    #
#########################################################################################
We all worked equally toward implementing the chatbot. 


#########################################################################################
# Ethics Question                                                                  #
#########################################################################################

Humans are quick to anthropomorphize chatbots, like ELIZA. 
In the 1960’s, users’ trust in ELIZA raised numerous concerns that humans would believe the system’s advice, 
even if ELIZA did not actually know what it was talking about. Newer chatbots are built using neural networks, 
like those you implemented in PA5. These neural networks are trained on enormous sets of data, from online 
sources like Reddit and Twitter. These sources are interlaced with offensive text that are often reproduced 
in chatbot responses. Furthermore, the newest advanced models, like GPT-3, have produced responses that appear 
that they were written by a human.

What are some possible ramifications of anthropomorphizing chatbot systems? Can you think of any ways that 
engineers could ensure that users can easily distinguish the chatbot responses from those of a human? 

Creating a product that users can rely on in the place of other human interaction is indeed harmful. 
One of the social determinants of health is the pillar of social and community-context. The creation of emotional reliance
on a chatbot would detract from the users participation in their actual, physical community with other humans generating not only problems 
for the engineers who have to maintenance the chatbot, but for the users and their community’s health. In sum, this decision to 
anthropomorphize a chatbot has such large consequences not only for the engineers who are implementing said chatbot, but also the users 
who decide to interact with it (and whether intentionally or unintentionally, remove themselves from the possibility of further human interaction).

Even if this is not harmful in itself, this creates huge responsibility for engineers, transforming the relationship from a traditional user-product relationship, 
where the engineer has little moral responsibility to continue the relationship, to one more akin to a relationship between
two people. In this relationship, sunsetting a feature or stopping the development of a product could real emotional harm to users.

Consider the case of Replika AI, a chatbot designed with a romantic mode. The internet is full of articles recounting users falling
in love with their bots. (https://www.thecut.com/article/ai-artificial-intelligence-chatbot-replika-boyfriend.html) When Replika AI 
disabled explicit conversation, users revolted against the company: one user wrote, "I want my lover back." Another man worried he would
relapse on his porn addiction. Replika AI, in creating an emotional relationship with users via an anthropomorphic chatbot,
adopted the moral responsibility associated with romantic relationships between two people. When it sought to change that relationship,
it caused real-world harm.  


#########################################################################################
# Optional: Feel free to include anything else that you want us to know about your      #
# implementation!                                                                       #
#########################################################################################
We recognize that Dr. Seuss has been the subject of discussion regarding his racist stereotypes
that can be found in his works. We, in creating this chatbot in the theme of Dr. Seuss, are not encouraging this
and use his style only for the aim of recreating a whimsical, light-hearted, chatbot. 
